{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Training dataset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265c2e9a56ca4f4ba6f12e083c7d9f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data processed\n",
      "Full-finetuning Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802128d592354fc8b9327f0bf798487a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.5517, 'learning_rate': 5e-06, 'epoch': 0.4}\n",
      "{'loss': 10.2475, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 3.5048, 'learning_rate': 1.5e-05, 'epoch': 1.2}\n",
      "{'loss': 0.8131, 'learning_rate': 2e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2477, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1537, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0952, 'learning_rate': 3.5e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0413, 'learning_rate': 4e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0264, 'learning_rate': 4.5e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoints/full_checkpoints/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0235, 'learning_rate': 5e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0177, 'learning_rate': 4.5e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0194, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0174, 'learning_rate': 3.5e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0159, 'learning_rate': 3e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0164, 'learning_rate': 2.5e-05, 'epoch': 6.0}\n",
      "{'loss': 0.0163, 'learning_rate': 2e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0162, 'learning_rate': 1.5e-05, 'epoch': 6.8}\n",
      "{'loss': 0.0157, 'learning_rate': 1e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0156, 'learning_rate': 5e-06, 'epoch': 7.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoints/full_checkpoints/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0154, 'learning_rate': 0.0, 'epoch': 8.0}\n",
      "{'train_runtime': 676.5376, 'train_samples_per_second': 11.825, 'train_steps_per_second': 1.478, 'train_loss': 1.5435467147231101, 'epoch': 8.0}\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "%run full_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Training dataset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937e8b63219149ffbcbc309740c2b86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastiandaws/miniconda3/envs/dsml_env/lib/python3.9/site-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data processed\n",
      "trainable params: 589,824 || all params: 61,096,448 || trainable%: 0.9653981848502878\n",
      "Lora-finetuning Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ee069821d24cbdb0b232ab76642b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.1701, 'learning_rate': 5e-06, 'epoch': 0.4}\n",
      "{'loss': 15.9859, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 15.2739, 'learning_rate': 1.5e-05, 'epoch': 1.2}\n",
      "{'loss': 13.7256, 'learning_rate': 2e-05, 'epoch': 1.6}\n",
      "{'loss': 10.7559, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n",
      "{'loss': 5.8938, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 1.9896, 'learning_rate': 3.5e-05, 'epoch': 2.8}\n",
      "{'loss': 0.8566, 'learning_rate': 4e-05, 'epoch': 3.2}\n",
      "{'loss': 0.3785, 'learning_rate': 4.5e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoints/lora_checkpoints/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2951, 'learning_rate': 5e-05, 'epoch': 4.0}\n",
      "{'loss': 0.2643, 'learning_rate': 4.5e-05, 'epoch': 4.4}\n",
      "{'loss': 0.2308, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.2154, 'learning_rate': 3.5e-05, 'epoch': 5.2}\n",
      "{'loss': 0.205, 'learning_rate': 3e-05, 'epoch': 5.6}\n",
      "{'loss': 0.1854, 'learning_rate': 2.5e-05, 'epoch': 6.0}\n",
      "{'loss': 0.1792, 'learning_rate': 2e-05, 'epoch': 6.4}\n",
      "{'loss': 0.1748, 'learning_rate': 1.5e-05, 'epoch': 6.8}\n",
      "{'loss': 0.1712, 'learning_rate': 1e-05, 'epoch': 7.2}\n",
      "{'loss': 0.1609, 'learning_rate': 5e-06, 'epoch': 7.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./checkpoints/lora_checkpoints/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1595, 'learning_rate': 0.0, 'epoch': 8.0}\n",
      "{'train_runtime': 505.8164, 'train_samples_per_second': 15.816, 'train_steps_per_second': 1.977, 'train_loss': 4.16357648897171, 'epoch': 8.0}\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "%run lora_finetuning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded\n",
      "Training dataset loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcd050db5df47049ee8af6017f34a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data processed\n",
      "Full-finetuning Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2942864dfa184e05a72626c73ba9ffca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.0136, 'learning_rate': 5e-06, 'epoch': 0.4}\n",
      "{'loss': 10.5745, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 3.6773, 'learning_rate': 1.5e-05, 'epoch': 1.2}\n",
      "{'loss': 0.7172, 'learning_rate': 2e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2279, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1412, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0684, 'learning_rate': 3.5e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0218, 'learning_rate': 4e-05, 'epoch': 3.2}\n",
      "{'loss': 0.014, 'learning_rate': 4.5e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0114, 'learning_rate': 5e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0079, 'learning_rate': 4.5e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0082, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0065, 'learning_rate': 3.5e-05, 'epoch': 5.2}\n",
      "{'loss': 0.005, 'learning_rate': 3e-05, 'epoch': 5.6}\n",
      "{'loss': 0.005, 'learning_rate': 2.5e-05, 'epoch': 6.0}\n",
      "{'loss': 0.0046, 'learning_rate': 2e-05, 'epoch': 6.4}\n",
      "{'loss': 0.005, 'learning_rate': 1.5e-05, 'epoch': 6.8}\n",
      "{'loss': 0.004, 'learning_rate': 1e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0044, 'learning_rate': 5e-06, 'epoch': 7.6}\n",
      "{'loss': 0.0044, 'learning_rate': 0.0, 'epoch': 8.0}\n",
      "{'train_runtime': 681.2792, 'train_samples_per_second': 11.743, 'train_steps_per_second': 1.468, 'train_loss': 1.5761074153482915, 'epoch': 8.0}\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "%run kd_full_finetuning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Knowledge Distillation Full-fintuned model on AquaRat...\n",
      "Testing... 25.0%\n",
      "Testing... 50.0%\n",
      "Testing... 75.0%\n",
      "Testing... 100.0%\n",
      "Knowledge Distillation Full-fintuned model AquaRat accuracy: 0.1\n",
      "KD_full Accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "%run eval_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
