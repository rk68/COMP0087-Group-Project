{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer, \n",
    "    T5ForConditionalGeneration, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    RobertaForMultipleChoice,\n",
    "    RobertaForQuestionAnswering,\n",
    "    RobertaTokenizer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import random\n",
    "import sympy as sp\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'aqua_rat'\n",
    "model_name = 'google-t5/t5-small'\n",
    "\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "reduced_dataset = dataset.shuffle(seed=42).select(range(2000))\n",
    "dataset_test = load_dataset(dataset_name, split='test')\n",
    "\n",
    "dataset2_name = 'ChilleD/SVAMP'\n",
    "dataset2i = load_dataset(dataset2_name, split='train')\n",
    "dataset2 = dataset2i.shuffle(seed=42).select(range(400))\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.save_pretrained('./before_finetuning')\n",
    "\n",
    "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "flan_t5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "roberta = RobertaForQuestionAnswering.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'SELECT * FROM LLM_results': no such table: LLM_results",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/dsml_env/lib/python3.9/site-packages/pandas/io/sql.py:2262\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2262\u001b[0m     \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: LLM_results",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m conn \u001b[38;5;241m=\u001b[39m sqlite3\u001b[38;5;241m.\u001b[39mconnect(database_name)\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM LLM_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml_env/lib/python3.9/site-packages/pandas/io/sql.py:486\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml_env/lib/python3.9/site-packages/pandas/io/sql.py:2326\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2317\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2324\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2326\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2327\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dsml_env/lib/python3.9/site-packages/pandas/io/sql.py:2274\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2273\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2274\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'SELECT * FROM LLM_results': no such table: LLM_results"
     ]
    }
   ],
   "source": [
    "database_name = '../teacher_llm_dataset.bd'\n",
    "conn = sqlite3.connect(database_name)\n",
    "query = \"SELECT * FROM LLM_results\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'options', 'rationale', 'correct'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['ID', 'Question', 'Type', 'Answer', 'Body', 'Equation'],\n",
      "    num_rows: 400\n",
      "})\n",
      "Dataset({\n",
      "    features: ['ID', 'Question', 'Type', 'Answer', 'Body', 'Equation'],\n",
      "    num_rows: 700\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'options', 'rationale', 'correct'],\n",
      "    num_rows: 254\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(reduced_dataset)\n",
    "print(dataset2)\n",
    "print(dataset2i)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aquarat(examples):\n",
    "    questions_and_options = [\n",
    "        f\"question: {q} options: {opts[0]} {opts[1]} {opts[2]} {opts[3]} {opts[4]}.\" \n",
    "        for q, opts in zip(examples[\"question\"], examples[\"options\"])]\n",
    "\n",
    "    correct_answers = [opts[ord(examples[\"correct\"][i]) - ord('A')] for i, opts in enumerate(examples[\"options\"])]\n",
    "\n",
    "    input_encodings = tokenizer(questions_and_options, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    target_encodings = tokenizer(correct_answers, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_encodings.input_ids,\n",
    "        \"attention_mask\": input_encodings.attention_mask,\n",
    "        \"labels\": target_encodings.input_ids\n",
    "    }\n",
    "\n",
    "def preprocess_svamp(examples):\n",
    "    questions_and_options = [\n",
    "        f\"question: {q} context: {bod}\" \n",
    "        for q, bod in zip(examples[\"Question\"], examples[\"Body\"])]\n",
    "\n",
    "    correct_answers = [str(ans) for ans in examples[\"Answer\"]]\n",
    "\n",
    "    input_encodings = tokenizer(questions_and_options, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    target_encodings = tokenizer(correct_answers, padding=\"max_length\", truncation=True, max_length=128)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_encodings.input_ids,\n",
    "        \"attention_mask\": input_encodings.attention_mask,\n",
    "        \"labels\": target_encodings.input_ids\n",
    "    }\n",
    "\n",
    "def ask_math_question(input_ids, model, tokenizer):\n",
    "    # Generate the output ids with the model\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)[0]\n",
    "    # Decode the generated ids to get the answer\n",
    "    answer = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'options', 'rationale', 'correct', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'options', 'rationale', 'correct', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 254\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "processed_dataset = reduced_dataset.map(preprocess_aquarat, batched=True)\n",
    "process_dataset_test = dataset_test.map(preprocess_aquarat, batched=True)\n",
    "\n",
    "print(processed_dataset)\n",
    "print(process_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63c8c45bc0c4801a071872838278a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.3583, 'learning_rate': 5e-06, 'epoch': 0.4}\n",
      "{'loss': 9.9169, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 2.9047, 'learning_rate': 1.5e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6761, 'learning_rate': 2e-05, 'epoch': 1.6}\n",
      "{'loss': 0.2116, 'learning_rate': 2.5e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1383, 'learning_rate': 3e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0628, 'learning_rate': 3.5e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0312, 'learning_rate': 4e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0232, 'learning_rate': 4.5e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0212, 'learning_rate': 5e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0194, 'learning_rate': 4.5e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0184, 'learning_rate': 4e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0179, 'learning_rate': 3.5e-05, 'epoch': 5.2}\n",
      "{'loss': 0.0165, 'learning_rate': 3e-05, 'epoch': 5.6}\n",
      "{'loss': 0.0166, 'learning_rate': 2.5e-05, 'epoch': 6.0}\n",
      "{'loss': 0.017, 'learning_rate': 2e-05, 'epoch': 6.4}\n",
      "{'loss': 0.0166, 'learning_rate': 1.5e-05, 'epoch': 6.8}\n",
      "{'loss': 0.0163, 'learning_rate': 1e-05, 'epoch': 7.2}\n",
      "{'loss': 0.0162, 'learning_rate': 5e-06, 'epoch': 7.6}\n",
      "{'loss': 0.0162, 'learning_rate': 0.0, 'epoch': 8.0}\n",
      "{'train_runtime': 1488.1258, 'train_samples_per_second': 10.752, 'train_steps_per_second': 0.672, 'train_loss': 1.4757732768654823, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    #load_best_model_at_end=True,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for 2 steps\n",
    "    max_grad_norm=1.0,  # Clip gradients to have a maximum norm of 1.0\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    # eval_dataset=processed_eval_dataset, # If you have an evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./after_finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random question from the training dataset\n",
    "sample_index = random.randint(0, len(processed_dataset) - 1)\n",
    "sample_question = processed_dataset[sample_index]\n",
    "\n",
    "# Load models for comparison\n",
    "model_before_finetuning = T5ForConditionalGeneration.from_pretrained('./before_finetuning')\n",
    "model_after_finetuning = T5ForConditionalGeneration.from_pretrained('./after_finetuning')\n",
    "\n",
    "# Generate answers to the selected question\n",
    "input_ids = torch.tensor(sample_question['input_ids']).unsqueeze(0)  # Add batch dimension\n",
    "pre_finetuning_answer = ask_math_question(input_ids, model_before_finetuning, tokenizer)\n",
    "post_finetuning_answer = ask_math_question(input_ids, model_after_finetuning, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question and options: question: How many campers went rowing in the afternoon? context: 46 campers went rowing on a day. 43 campers went rowing in the morning and some more campers went rowing in the afternoon.\n",
      "Answer before fine-tuning: 43\n",
      "Answer after fine-tuning: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Display the question and compare answers\n",
    "print(\"Question and options:\", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "print(\"Answer before fine-tuning:\", pre_finetuning_answer)\n",
    "print(\"Answer after fine-tuning:\", post_finetuning_answer)\n",
    "print(f'Correct answer: {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(model, tokenizer, prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output\n",
    "\n",
    "print(generate_answer(model_before_finetuning, tokenizer, 'question: what is x + y? context: x = 4, y = 3.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['ID', 'Question', 'Type', 'Answer', 'Body', 'Equation'],\n",
      "    num_rows: 700\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset2_name = 'ChilleD/SVAMP'\n",
    "dataset2 = load_dataset(dataset2_name, split='train')\n",
    "questions2 = dataset2['Question']\n",
    "body2 = dataset2['Body']\n",
    "\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's extracted answer: <s>How many days did he take to finish the book?</s></s>Frank was reading through his favorite book, the book had 392 pages and he read 14 pages per\n"
     ]
    }
   ],
   "source": [
    "question_rob = \"How many days did he take to finish the book?\"\n",
    "context_rob = \"Frank was reading through his favorite book, the book had 392 pages and he read 14 pages per day.\"\n",
    "\n",
    "# Encode the inputs\n",
    "inputs = roberta_tokenizer(question_rob, context_rob, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# Get model's answer span predictions\n",
    "outputs = roberta(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "\n",
    "# Determine the start and end positions of the answer\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "# Decode the predicted answer\n",
    "answer_tokens = input_ids[0, answer_start:answer_end]\n",
    "answer = roberta_tokenizer.decode(answer_tokens)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Model's extracted answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = []\n",
    "flan_t5_list = []\n",
    "t5_list = []\n",
    "correct_list = []\n",
    "t5_tuned = []\n",
    "\n",
    "for qid in range(10):\n",
    "    prompt = f'question: What is x+y? context: x=2 y=3.'\n",
    "    prompt2 = tokenizer.decode(processed_dataset['input_ids'][qid], skip_special_tokens=True)\n",
    "    answer2 = tokenizer.decode(processed_dataset['labels'][qid], skip_special_tokens=True)\n",
    "    promptflan = flan_tokenizer.decode(processed_dataset['input_ids'][qid], skip_special_tokens=True)\n",
    "    #promptroberta = roberta_tokenizer.decode(processed_dataset['input_ids'][qid], skip_special_tokens=True)\n",
    "\n",
    "    prompt_list.append(prompt2)\n",
    "    flan_t5_list.append(generate_answer(flan_t5, flan_tokenizer, promptflan))\n",
    "    t5_list.append(generate_answer(model_before_finetuning, tokenizer, prompt2))\n",
    "    t5_tuned.append(generate_answer(model_after_finetuning, tokenizer, prompt2))\n",
    "    correct_list.append(answer2)\n",
    "\n",
    "#print(prompt2)\n",
    "#print(f'Flan-T5: {generate_answer(flan_t5, flan_tokenizer, promptflan)}')\n",
    "#print(f'RoBERTa: {generate_answer(roberta, roberta_tokenizer, promptroberta)}')\n",
    "#print(f'T5 before Fine-tuning: {generate_answer(model_before_finetuning, tokenizer, prompt2)}')\n",
    "#print(f'T5 fter Finetuning: {generate_answer(model_after_finetuning, tokenizer, prompt2)}')\n",
    "#print(f'Correct Answer: {answer2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: There are r red ball, b blue ball and w white ball in a bag. What is the ratio of the number of blue ball to the total no. of ball in terms of r, b and w.? options: A)r / (r + b + w) B)r * (r + b + w) C)(r + b + w) D)r / (r + b ) E)r / (b + w).\n",
      "Correct Answer: A)r / (r + b + w)\n",
      "Flan-T5: D\n",
      "T5 before Fine-tuning: r, b and w\n",
      "T5 after Fine-tuning: A)r / (r + b + w)\n",
      "\n",
      "question: From (1, 2, 3, 4, 5, 6), one number is picked out and replaced and one number is picked out again. If the sum of the 2 numbers is 9, what is the probability that the 2 numbers included the number 5? options: A)1/2 B)2/5 C)3/10 D)3/5 E)1/4.\n",
      "Correct Answer: A)1/2\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: one number is picked out and replaced\n",
      "T5 after Fine-tuning: B)2/5\n",
      "\n",
      "question: Suresh started a business, investing Rs.18000. After 3 months and 4 months respectively, Rohan and Sudhir joined him with capitals of 12000 and 9000. At the end of the year the total profit was Rs.4048. What is the difference between Rohan’s and Sudhir’s share in the profit? options: A)s.345 B)s.350 C)s.352 D)s.362 E)s.368.\n",
      "Correct Answer: E)s.368\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: capitals of 12000 and 9000\n",
      "T5 after Fine-tuning: A)s.345\n",
      "\n",
      "question: Find? /20 = 4? options: A)76 B)6400 C)304 D)1296 E)None.\n",
      "Correct Answer: B)6400\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: options: A)76 B)6400 C)304 D)1296 E)None\n",
      "T5 after Fine-tuning: B)6400\n",
      "\n",
      "question: A doctor prescribed 18 cubic centimeters of a certain drug to a patient whose body weight was 135 pounds. If the typical dosage is 2 cubic centimeters per 15 pounds of the body weight, by what percent was the prescribed dosage greater than the typical dosage? options: A)8% B)9% C)11% D)12.5% E)14.8%.\n",
      "Correct Answer: C)11%\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 135 pounds\n",
      "T5 after Fine-tuning: D)12.5%\n",
      "\n",
      "question: A and B together can do a work in7 days. If A alone can do it in 14 days. In how many days can B alone do it? options: A)14 B)10 C)21 D)20 E)25.\n",
      "Correct Answer: A)14\n",
      "Flan-T5: A\n",
      "T5 before Fine-tuning: 14 days\n",
      "T5 after Fine-tuning: B)10\n",
      "\n",
      "question: If each edge of cube increased by 10%, the percentage increase in options: A)21 B)22 C)25 D)19 E)15.\n",
      "Correct Answer: A)21\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: A)21 B)22 C)25 D)19 E)15\n",
      "T5 after Fine-tuning: E)15\n",
      "\n",
      "question: A cistern has a leak which empty it in 8 hrs, A tap is turned on which admits 6 liters a minute into the cistern, and it is now emptied in 12 hours, How many liters does the cistern hold? options: A)8640 B)8740 C)8840 D)8540 E)8940.\n",
      "Correct Answer: A)8640\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: 6 liters a minute\n",
      "T5 after Fine-tuning: C)8840\n",
      "\n",
      "question: A freight elevator can carry a maximum load of 1100 pounds. Sean, who weighs 200 pounds, is in the elevator with two packages weighing 150 pounds and 280 pounds. If he needs to fit three more packages in the elevator that weigh as much as possible without exceeding the elevator limit, what is the difference between their average and the average of the two packages already in the elevator? options: A)59 B)85 C)190 D)215 E)210.\n",
      "Correct Answer: A)59\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 280 pounds\n",
      "T5 after Fine-tuning: C)190\n",
      "\n",
      "question: What is the area of a square field whose diagonal of length 20 m? options: A)150 B)300 C)200 D)450 E)600.\n",
      "Correct Answer: C)200\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 20 m\n",
      "T5 after Fine-tuning: C)200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(prompt_list[i])\n",
    "    print(f'Correct Answer: {correct_list[i]}')\n",
    "    print(f'Flan-T5: {flan_t5_list[i]}')\n",
    "    print(f'T5 before Fine-tuning: {t5_list[i]}')\n",
    "    print(f'T5 after Fine-tuning: {t5_tuned[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "\n",
    "prompt_list_test = []\n",
    "flan_t5_list_test = []\n",
    "t5_list_test = []\n",
    "correct_list_test = []\n",
    "t5_tuned_test = []\n",
    "\n",
    "for qid in range(30):\n",
    "    prompt2 = tokenizer.decode(process_dataset_test['input_ids'][qid], skip_special_tokens=True)\n",
    "    answer2 = tokenizer.decode(process_dataset_test['labels'][qid], skip_special_tokens=True)\n",
    "    promptflan = flan_tokenizer.decode(process_dataset_test['input_ids'][qid], skip_special_tokens=True)\n",
    "    #promptroberta = roberta_tokenizer.decode(processed_dataset['input_ids'][qid], skip_special_tokens=True)\n",
    "\n",
    "    prompt_list_test.append(prompt2)\n",
    "    flan_t5_list_test.append(generate_answer(flan_t5, flan_tokenizer, promptflan))\n",
    "    t5_list_test.append(generate_answer(model_before_finetuning, tokenizer, prompt2))\n",
    "    t5_tuned_test.append(generate_answer(model_after_finetuning, tokenizer, prompt2))\n",
    "    correct_list_test.append(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: A car is being driven, in a straight line and at a uniform speed, towards the base of a vertical tower. The top of the tower is observed from the car and, in the process, it takes 10 minutes for the angle of elevation to change from 45° to 60°. After how much more time will this car reach the base of the tower? options: A)5(3 + 1) B)6(3 + 2) C)7(3 – 1) D)8(3 – 2) E)None of these.\n",
      "Correct Answer: A)5(3 + 1)\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: a uniform speed\n",
      "T5 after Fine-tuning: C)7(3 – 1)\n",
      "\n",
      "question: The original price of an item is discounted 22%. A customer buys the item at this discounted price using a $20-off coupon. There is no tax on the item, and this was the only item the customer bought. If the customer paid $1.90 more than half the original price of the item, what was the original price of the item? options: A)$61 B)$65 C)$67.40 D)$70 E)$78.20.\n",
      "Correct Answer: E)$78.20\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 78.20\n",
      "T5 after Fine-tuning: C)$67.40\n",
      "\n",
      "question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12? options: A)36 B)15 C)17 D)5 E)7.\n",
      "Correct Answer: A)36\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: options: A)36 B)15 C)17 D)5 E)7\n",
      "T5 after Fine-tuning: C)17\n",
      "\n",
      "question: If the probability that Stock A will increase in value during the next month is 0.56, and the probability that Stock B will increase in value during the next month is 0.74. What is the greatest value for the probability that neither of these two events will occur? options: A)0.22 B)0.26 C)0.37 D)0.46 E)0.63.\n",
      "Correct Answer: B)0.26\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: the probability that Stock B will increase in value during the next month is 0.74\n",
      "T5 after Fine-tuning: C)0.37\n",
      "\n",
      "question: A trader sold an article at a profit of 20% for Rs.360. What is the cost price of the article? options: A)270 B)300 C)280 D)320 E)315.\n",
      "Correct Answer: B)300\n",
      "Flan-T5: D\n",
      "T5 before Fine-tuning: Rs.360\n",
      "T5 after Fine-tuning: C)280\n",
      "\n",
      "question: 20 marbles were pulled out of a bag of only white marbles, painted black, and then put back in. Then, another 20 marbles were pulled out, of which 1 was black, after which they were all returned to the bag. If the percentage of black marbles pulled out the second time represents their percentage in the bag, how many marbles in total Q does the bag currently hold? options: A)40 B)200 C)380 D)400 E)3200.\n",
      "Correct Answer: D)400\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 1 was black\n",
      "T5 after Fine-tuning: A)40\n",
      "\n",
      "question: Find the total no. of distinct bike no.'s that can beformed using 2 letters followed by 2 no.'s. How many letters need to be distinct? options: A)74453 B)64543 C)74325 D)65000 E)97656.\n",
      "Correct Answer: D)65000\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: bike no.'s\n",
      "T5 after Fine-tuning: B)64543\n",
      "\n",
      "question: A train running at a speed of 100 miles/hour, takes 10 hours to reach its destination. After covering quarter of the distance, it starts raining and the train has to be slowed to speed of 75 miles/hour. What is the total journey duration? options: A)10 B)11.5 C)12.5 D)13.5 E)15.\n",
      "Correct Answer: C)12.5\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 75 miles/hour\n",
      "T5 after Fine-tuning: B)11.5\n",
      "\n",
      "question: Of the 200 students in a school, at least 45% attended the prom night and at least 35% took part in the debating session. What is the maximum number of students who could have neither attended the prom night nor the debating session? options: A)27 B)81 C)90 D)99 E)110.\n",
      "Correct Answer: E)110\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: maximum number of students who could have neither attended the prom night nor the debating session?\n",
      "T5 after Fine-tuning: C)90\n",
      "\n",
      "question: A sales person gets a 10% commission on each sale he makes. How many sales of $250 each must he make in order to reach a salary of at least $1000? options: A)15 B)24 C)25 D)40 E)52.\n",
      "Correct Answer: D)40\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: a 10% commission\n",
      "T5 after Fine-tuning: C)25\n",
      "\n",
      "question: A company produces 420 units of a particular computer component every month, at a production cost to the company of $110 per component, and sells all of the components by the end of each month. What is the minimum selling price per component that will guarantee that the yearly profit (revenue from sales minus production costs) will be at least $626,400? options: A)226 B)230 C)240 D)260 E)280.\n",
      "Correct Answer: B)230\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: options: A)226 B)230 C)240 D)260 E)280\n",
      "T5 after Fine-tuning: B)230\n",
      "\n",
      "question: At a certain factory, 10 percent of the staplers produced on Monday were defective and 2 percent of the non-defective staplers were rejected by mistake. If 72 of the non-defective staplers were rejected, what was the number of staplers produced that day? options: A)4,000 B)4,200 C)4,500 D)4,800 E)5,000.\n",
      "Correct Answer: A)4,000\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 72\n",
      "T5 after Fine-tuning: B)4,200\n",
      "\n",
      "question: Machine A puts out a yo-yo every 6 minutes. Machine B puts out a yo-yo every 9 minutes. After how many minutes will they have produced 10 yo-yos? options: A)24 minutes B)32 minutes C)36 minutes D)64 minutes E)72 minutes.\n",
      "Correct Answer: C)36 minutes\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: 9 minutes\n",
      "T5 after Fine-tuning: B)32 minutes\n",
      "\n",
      "question: Add: +45 and -30 options: A)-30 B)+30 C)0 D)15 E)-15.\n",
      "Correct Answer: D)15\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: +45\n",
      "T5 after Fine-tuning: D)15\n",
      "\n",
      "question: In how many ways can the letters of the word \"PROBLEC\" be rearranged to make 7 letter words such that none of the letters repeat? options: A)2! B)3! C)7! D)8! E)9!.\n",
      "Correct Answer: C)7!\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 7\n",
      "T5 after Fine-tuning: C)7!\n",
      "\n",
      "question: Let A and B be independent events with P (A) = 0.2 and P(B) = 0.8. Find P(A/B)? options: A)0.2 B)0.4 C)0.6 D)1.2 E)1.5.\n",
      "Correct Answer: A)0.2\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: P(A/B) = 0.8\n",
      "T5 after Fine-tuning: C)0.6\n",
      "\n",
      "question: Consider there is an staircase elevator and you are coming down. If you walk 20 steps and stop, then you reach bottom in 10 minutes. If you walk 10 steps and stop, you reach to the ground in 20 minutes. What is the speed of the elevator? options: A)1 step/minute B)2 step/minute C)3 step/minute D)4 step/minute E)None of the above.\n",
      "Correct Answer: A)1 step/minute\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: if you walk 20 steps and stop, then you reach bottom in 10 minutes.\n",
      "T5 after Fine-tuning: A)1 step/minute\n",
      "\n",
      "question: Last year, a Home Appliance Store sold an average(arithmetic mean) of 42 microwave ovens per month. In the first 10 months of this year,the store has sold an average(arithmetic mean) of only 20 microwave ovens per month. What was the average number of microwave ovens sold per month during the entire 22 months period? options: A)21 B)30 C)31 D)32 E)None of the above.\n",
      "Correct Answer: D)32\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 20 microwave ovens per month\n",
      "T5 after Fine-tuning: B)30\n",
      "\n",
      "question: An exam is given in a certain class. The average (arithmetic mean) of the highest score and the lowest score is equal to x. If the average score for the entire class is equal to y and there are z students in the class, where z > 5, then in terms of x, y, and z, what is the average score for the class excluding the highest and lowest scorers? options: A)(zy – 2x)/z B)(zy – 2)/z C)(zx – y)/(z – 2) D)(zy – 2x)/(z -2) E)(zy – x)/(z + 2).\n",
      "Correct Answer: D)(zy – 2x)/(z -2)\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: x, y\n",
      "T5 after Fine-tuning: D)(zy – 2x)/(z -2)\n",
      "\n",
      "question: [5 +?  19 - 15 - 7]/[13  13 - 156] = 6 options: A)4 B)4.5 C)5 D)5.5 E)6.5.\n",
      "Correct Answer: C)5\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: 13 13 - 156] = 6 options: A)4 B)4.5 C)5 D)5.5 E)6.5\n",
      "T5 after Fine-tuning: C)5\n",
      "\n",
      "question: A grocer makes a 25% profit on the selling price for each bag of flour it sells. If he sells each bag for $100 and makes $3,000 in profit, how many bags did he sell? options: A)12 B)16 C)24 D)30 E)40.\n",
      "Correct Answer: C)24\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: $3,000\n",
      "T5 after Fine-tuning: C)24\n",
      "\n",
      "question: Alex and Jacob works at a toy shop that make toys. Alex takes 7 hours to make a toy, and Jacob takes 9 hours to make a toy. During a month, both of them makes 35 toys in total. If both of them have worked for almost similar number of hours how many toys have been prepared by Jacob? options: A)15 B)16 C)17 D)18 E)19.\n",
      "Correct Answer: A)15\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: Jacob takes 9 hours to make a toy\n",
      "T5 after Fine-tuning: B)16\n",
      "\n",
      "question: John likes to have lightly flavored tea every evening. In a 50% strong milk tea, he replaces 15% of it with milk twice. Then, he replaces 10 percent of the resultant solution with more milk. What is the final concentration of tea John drinks? options: A)15.38% B)42% C)39.86% D)22.35% E)32.51%.\n",
      "Correct Answer: E)32.51%\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: he replaces 10% of it with milk twice\n",
      "T5 after Fine-tuning: C)39.86%\n",
      "\n",
      "question: In a class 1/16 of the students study math, 1/10 of the students study bio, 1/8 of the students study english. The total number of students is a 4 digit number. Find the diffrence between maximum number of students and minimum number of students. options: A)8880 B)8870 C)8890 D)7890 E)6780.\n",
      "Correct Answer: A)8880\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: math\n",
      "T5 after Fine-tuning: C)8890\n",
      "\n",
      "question: On a normal day Bill usually averages about 15 mph when riding his bicycle. On a windy day, his speed is reduced by 4 mph. How far can Bill travel on a windy day in 21 minutes? Round to the nearest hundredth. options: A)2 miles B)2.25 miles C)3.25 miles D)3.85 miles E)2.85 miles.\n",
      "Correct Answer: D)3.85 miles\n",
      "Flan-T5: C\n",
      "T5 before Fine-tuning: his speed is reduced by 4 mph\n",
      "T5 after Fine-tuning: B)2.25 miles\n",
      "\n",
      "question: A retailer sold an appliance for 40 percent above cost, which represented a gross profit of $20.00. For what price did the retailer sell the appliance? options: A)$27.30 B)$51.00 C)$63.00 D)$70.00 E)$91.00.\n",
      "Correct Answer: D)$70.00\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: a gross profit of $20.00\n",
      "T5 after Fine-tuning: A)$27.30\n",
      "\n",
      "question: At 6% per annum simple interest, Rahul borrowed Rs. 500. What amount will he pay to clear the debt after 4 years options: A)750 B)700 C)620 D)600 E)None of these.\n",
      "Correct Answer: C)620\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: Rs. 500\n",
      "T5 after Fine-tuning: C)620\n",
      "\n",
      "question: A computer routine was developed to generate two numbers (x,y) the first being a random number between 0 and 100 inclusive, and the second being less than or equal to the square root of the first. Each of the following pair satisfies the routine except options: A)(99,10) B)(85,9) C)(50,7) D)(1,1) E)(1,0).\n",
      "Correct Answer: A)(99,10)\n",
      "Flan-T5: D\n",
      "T5 before Fine-tuning: each of the following pair satisfies the routine except options\n",
      "T5 after Fine-tuning: B)(85,9)\n",
      "\n",
      "question: A jeep travels a certain distance taking 6 hours in the forward journey. During the return journey, it increased its speed by 12km/hr and took 4 hours. What is the distance travelled by the jeep? options: A)126km B)144km C)127km D)228km E)128km.\n",
      "Correct Answer: B)144km\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: options: A)126km B)144km C)127km D)228km E)128km\n",
      "T5 after Fine-tuning: B)144km\n",
      "\n",
      "question: When I was 2 years old, my brother was half my age. Now I am 60 years old, how old is my brother? options: A)A)59 B)B)69 C)C)79 D)D)89 E)E)99.\n",
      "Correct Answer: A)A)59\n",
      "Flan-T5: B\n",
      "T5 before Fine-tuning: half my age\n",
      "T5 after Fine-tuning: D)D)89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(prompt_list_test[i])\n",
    "    print(f'Correct Answer: {correct_list_test[i]}')\n",
    "    print(f'Flan-T5: {flan_t5_list_test[i]}')\n",
    "    print(f'T5 before Fine-tuning: {t5_list_test[i]}')\n",
    "    print(f'T5 after Fine-tuning: {t5_tuned_test[i]}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative results can be good for the result \n",
    "Probaly train on more samples than 200 to 400\n",
    "\n",
    "multiple choice answer from a model:\n",
    "BERT instead of seq2seq --> Score for every answer\n",
    "T5 (seq2seq) --> prompt to gen multiple outputs and select most freq answer\n",
    "Could ask model: How much from 0 to 100 do you believe this answer is correct?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
